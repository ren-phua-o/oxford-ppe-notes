# Multivariate Calculus Rough Notes

## Continuity
- **A function $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is continuous at $\mathbf{\overrightarrow{x}}$ iff $\forall\epsilon>0:\exists\delta>0:$ if $||\mathbf{\overrightarrow{y}}-\mathbf{\overrightarrow{x}}||<\delta$ then $||f(\mathbf{\overrightarrow{y}})-f(\mathbf{\overrightarrow{x}})||<\epsilon$.** In other words, a function $f$ is continuous at some point $\mathbf{\overrightarrow{x}}$ iff for all open balls centred around $f(\mathbf{\overrightarrow{x}})$, there is some open ball centred around $\mathbf{\overrightarrow{x}}$ such that $f$ maps all points in the latter circle to some point in the former circle. Informally, points near $\mathbf{\overrightarrow{x}}$ map to points near $f(\mathbf{\overrightarrow{x}})$. (See Moreno de Barreda, 2023 - Lecture on Multivariate Calculus, p. 8 for intuition.)
	- This definition is generally useful for proving continuity.
- Equivalently, a function $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is continuous at $\mathbf{\overrightarrow{x}}$ if for any sequence $\{\mathbf{\overrightarrow{x}}_k\}_{k=1}^{\infty}$ converging to $\mathbf{\overrightarrow{x}}$, $\{f(\mathbf{\overrightarrow{x}}_k)\}_{k=1}^{\infty}$ converges to $f(\mathbf{\overrightarrow{x}})$.
	- This definition is generally useful for proving non-continuity.

### [Limit Laws](https://openstax.org/books/calculus-volume-1/pages/2-3-the-limit-laws)
- $\lim_{x\rightarrow a}x=a,\lim_{x\rightarrow a}c=c$.
- $\lim_{x\rightarrow a}[f(x)+g(x)]=\lim_{x\rightarrow a}f(x)+\lim_{x\rightarrow a}g(x)$ and likewise for subtraction, multiplication, and division.
- $\lim_{x\rightarrow a}cf(x)=c\lim_{x\rightarrow a}f(x),\lim_{x\rightarrow a}[f(x)]^n=[\lim_{x\rightarrow a}f(x)]^n$.
- Common techniques for evaluating $\lim_{x\rightarrow a}\frac{f(x)}{g(x)}$ where $g(a)=0$ include factoring the numerator and denominator, multiplying by a conjugate, and applying L'Hopital's rule: $\lim_{x\rightarrow a}\frac{f(x)}{g(x)}=\lim_{x\rightarrow a}\frac{f'(x)}{g'(x)}$.

### Common Results
- Consider $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ and $g:\mathbb{R}^n\rightarrow\mathbb{R}^m$. If each of $f$ and $g$ is continuous at $\mathbf{\overrightarrow{x}}$, then each of $f+g$, $f-g$, $f\times g$ is continuous at $\mathbf{\overrightarrow{x}}$. 
- Consider $f:\mathbb{R}^n\rightarrow\mathbb{R}$ and $g:\mathbb{R}^n\rightarrow\mathbb{R}$. If $g(\mathbf{\overrightarrow{x}})\neq0$, and each of $f$ and $g$ is continuous at $\mathbf{\overrightarrow{x}}$, then $f/g$ is continuous at $\mathbf{\overrightarrow{x}}$.
- Consider $f=(f_1,f_2,\ldots,f_m):\mathbb{R}^n\rightarrow\mathbb{R}^m$. $f$ is continuous at $\mathbf{\overrightarrow{x}}$ iff each of $f_1,f_2,\ldots,f_m$ is continuous at $\mathbf{\overrightarrow{x}}$.
- Consider $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ and $g:\mathbb{R}^m\rightarrow\mathbb{R}^k$. If $f$ is continuous at $\mathbf{\overrightarrow{x}}$ and $g$ is continuous at $f(\mathbf{\overrightarrow{x}})$, then $g\circ f(\mathbf{\overrightarrow{x}})\equiv g(f(\mathbf{\overrightarrow{x}}))$ is continuous at $\mathbf{\overrightarrow{x}}$.

### Weierstrass Extreme Value Theorem
- The Weierstrass extreme value theorem states that a real-valued function $f:S\rightarrow\mathbb{R}$ attains a maximum and a minimum iff $f$ is continuous for all $\mathbf{\overrightarrow{x}}\in S$ and $S$ is a compact (i.e. closed and bounded) set.
	- A set is closed iff any sequence of points, each in the set, converges to some point in the set. Informally, a set is closed if the boundaries of that set are contained in that set. For example, $[0,1]$ is a closed set and $(0,1)$ is an open set.
	- A set is bounded iff there exists some ball that contains that set. Equivalently, a set is bounded iff there exists some upper bound and some lower bound of the set in each dimension. For example, $[0,\infty)$ is not bounded.

## Differentiation
- The $k^{th}$ partial derivative of $f:\mathbb{R}^n\rightarrow\mathbb{R}$ at $\mathbf{\overrightarrow{x}}\in\mathbb{R}^n$ is defined as $\frac{\partial f}{\partial x_k}(\mathbf{\overrightarrow{x}})=\lim_{h\rightarrow0}\frac{f(x_1,\ldots,x_k+h,\ldots,x_n)-f(\mathbf{\overrightarrow{x}})}{h}$. If this limit does not exist, then the $k^{th}$ partial derivative at $\mathbf{\overrightarrow{x}}$, $\frac{\partial f}{\partial x_k}(\mathbf{\overrightarrow{x}})$ does not exist.
- The $k^{th}$ partial derivative of $f$ is denoted by each of $\frac{\partial f}{\partial x_k},f_k,f_{x_k},\partial_kf,\partial_{x_k}f,D_kf$.
- The second-order partial derivative $\frac{\partial^2f}{\partial x_i\partial x_j}$ of the function $f$ is equal to $\frac{\partial}{\partial x_i}\frac{\partial f}{\partial x_j}$.
- The second-order partial derivative $\frac{\partial^2f}{\partial x_i\partial x_j}$ is also denoted by each of $\partial_i\partial_jf,f_{ij}$.
- Young's theorem states that if each of $\partial_i\partial_jf$ and $\partial_j\partial_if$ exists and is continuous, the two are equal.

### Differentiability
- The function $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is differentiable at $\mathbf{\overrightarrow{x}}$ iff it can be approximated by a linear function around $\mathbf{\overrightarrow{x}}$. Formally, this is iff there exists a $m\times n$ matrix $A$ such that $\lim_{||\mathbf{\overrightarrow{h}}||\rightarrow0}\frac{||f(\mathbf{\overrightarrow{x}}+\mathbf{\overrightarrow{h}})-f(\mathbf{\overrightarrow{x}})-A\mathbf{\overrightarrow{h}}||}{||\mathbf{\overrightarrow{h}}||}=0$ (where $\mathbf{\overrightarrow{h}}$ is $n$-dimensional).
- If $f$ is differentiable at $\mathbf{\overrightarrow{x}}$, i.e. there exists such $A$, then $f$ is continuous at $\mathbf{\overrightarrow{x}}$ and the unique such $A$ is the Jacobian matrix $Df$ of $f$, i.e. $f(\mathbf{\overrightarrow{x}}+\mathbf{\overrightarrow{h}})\approx f(\mathbf{\overrightarrow{x}})+Df(\mathbf{\overrightarrow{x}})\cdot\mathbf{\overrightarrow{h}}\Rightarrow f(\mathbf{\overrightarrow{x'}})\approx f(\mathbf{\overrightarrow{x}})+Df(\mathbf{\overrightarrow{x}})\cdot(\mathbf{\overrightarrow{x'}}-\mathbf{\overrightarrow{x}})$ near $\mathbf{\overrightarrow{x}}$.
	- From the above, $\lim_{||\mathbf{\overrightarrow{h}}||\rightarrow0}\frac{||f(\mathbf{\overrightarrow{x}}+\mathbf{\overrightarrow{h}})-f(\mathbf{\overrightarrow{x}})-Df(\mathbf{\overrightarrow{x}})\mathbf{\overrightarrow{h}}||}{||\mathbf{\overrightarrow{h}}||}=0$ is necessary for the differentiability of $f$.
- **A function $f$ is differentiable at $\mathbf{\overrightarrow{x}}$ if (but not only if) (1) in a neighbourhood of $\mathbf{\overrightarrow{x}}$ all partial derivatives of $f$ exist, and (2) all partial derivatives of $f$ are continuous at $\mathbf{\overrightarrow{x}}$.**
	- This is a sufficient (but not necessary) condition
- **A function $f$ is continuously differentiable (i.e. $C^1$) iff at all points in the domain of $f$, the partial derivatives of $f$ exist, and are continuous.**
- **A function $f$ is twice continuously differentiable (i.e. $C^2)$ iff at all points in the domain of $f$, the second-order partial derivatives of $f$ exist, and are continuous.
- **$C^2\Rightarrow C^1\Rightarrow$ differentiable (at all points) $\Rightarrow$ continuous (at all points).**
	- Then, $C^1$ is a sufficient condition for differentiability and continuity, and continuity is a necessary condition for differentiability and $C^1$.
- **Differentiable (at some point) $\Rightarrow$ continuous (at that point).**

### Gradient Vector
- The gradient vector $\nabla f$ of the function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is well-defined iff $\forall k\in\{1,\ldots,n\}:\partial_kf$ exists. If $\nabla f$ exists, $\nabla f=\begin{pmatrix}\partial_1f\\\partial_2f\\\vdots\\\partial_nf\end{pmatrix}$.
- $\nabla f(\mathbf{\overrightarrow{x}})$ is a vector that points in the direction from $\mathbf{\overrightarrow{x}}$ in which the rate of change of $f$ is greatest. The gradient vector at some point is perpendicular, at that point, to the level curve that intersects that point. (See Moreno de Barreda, 2023 - Lecture on Multivariate Calculus, p. 17 for illustration.)

## Directional Derivative
- The directional derivative of a function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ at $\mathbf{\overrightarrow{x}}$ in direction $\mathbf{\overrightarrow{v}}$ is defined as $Df(\mathbf{\overrightarrow{x}},\mathbf{\overrightarrow{v}})=\lim_{t\rightarrow0}\frac{f(\mathbf{\overrightarrow{x}}+t\mathbf{\overrightarrow{v}})-f(\mathbf{\overrightarrow{x}})}{t}$.
	- For a $C^1$ function, this is equal to $\nabla f(\mathbf{\overrightarrow{x}})\cdot\mathbf{\overrightarrow{v}}$ .

### Jacobian Matrix
 The Jacobian matrix of the function $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is the $m\times n$ matrix $Df(\mathbf{\overrightarrow{x}})=\begin{pmatrix}\partial_1f_1&\partial_2f_1&\ldots&\partial_nf_1\\\partial_1f_2&\partial_2f_2&\ldots&\partial_nf_2\\\vdots&\vdots&\ddots&\vdots\\\partial_1f_m&\partial_2f_m&\ldots&\partial_nf_m\end{pmatrix}$ (if each partial derivative exists). Each row of the Jacobian matrix consists of the partial derivatives of some $f_i$ and each column of the Jacobian matrix consists of the partial derivatives of $f$ with respect to some $x_j$.

### Hessian Matrix
- The Hessian matrix of the function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is $D^2f(\mathbf{\overrightarrow{x}})=\begin{pmatrix}\partial_1\partial_1f&\partial_1\partial_2f&\ldots&\partial_1\partial_nf\\\partial_2\partial_1f&\partial_2\partial_2f&\ldots&\partial_2\partial_nf\\\vdots&\vdots&\ddots&\vdots\\\partial_n\partial_1f&\partial_n\partial_2f&\ldots&\partial_n\partial_nf\end{pmatrix}$.
- If $f$ is $C^2$, then $D^2f(\mathbf{\overrightarrow{x}})$ is a symmetric matrix.

### Chain Rule
- If each of $f:\mathbb{R}^n\rightarrow\mathbb{R}$ and $\mathbf{\overrightarrow{x}}:\mathbb{R}^m\rightarrow\mathbb{R}^n$ is a $C^1$ function, then $Z:\mathbb{R}^m\rightarrow\mathbb{R}$ such that $Z(\mathbf{\overrightarrow{t}})=f(\mathbf{\overrightarrow{x}}(\mathbf{\overrightarrow{t}}))$ is $C^1$ and $\frac{\partial z}{\partial t_k}=\frac{\partial f}{\partial x_1}\frac{\partial x_1}{\partial t_k}+\ldots+\frac{\partial f}{\partial x_n}\frac{\partial x_n}{\partial t_k}$ for $1\leq k\leq m$.

## Taylor Approximation
- Taylor's (first order) theorem states that if function $f:U\rightarrow\mathbb{R}$ is $C^1$ and $U$ is an open subset of $\mathbb{R}^n$, then $\forall\mathbf{\overrightarrow{a}},\mathbf{\overrightarrow{x}}\in U:f(\mathbf{\overrightarrow{x}})=f(\mathbf{\overrightarrow{a}})+Df(\mathbf{\overrightarrow{a}})(\mathbf{\overrightarrow{x}}-\mathbf{\overrightarrow{a}})+R_1(\mathbf{\overrightarrow{x}},\mathbf{\overrightarrow{a}})$, where $\lim_{\mathbf{\overrightarrow{x}}\rightarrow\mathbf{\overrightarrow{a}}}\frac{R_1(\mathbf{\overrightarrow{x}},\mathbf{\overrightarrow{a}})}{||(\mathbf{\overrightarrow{x}}-\mathbf{\overrightarrow{a}})||}\rightarrow0$.
	- Informally, any $f(\mathbf{\overrightarrow{x}})$ is well approximated by $f(\mathbf{\overrightarrow{a}})+Df(\mathbf{\overrightarrow{a}})(\mathbf{\overrightarrow{x}}-\mathbf{\overrightarrow{a}})$. **Intuitively, the first-order Taylor approximation $f(\mathbf{\overrightarrow{x}})\approx f(\mathbf{\overrightarrow{a}})+Df(\mathbf{\overrightarrow{a}})(\mathbf{\overrightarrow{x}}-\mathbf{\overrightarrow{a}})$ gives the equation of the plane that is tangent to the surface $z=f(\mathbf{\overrightarrow{x}})$ at $\mathbf{\overrightarrow{a}}$.**
- Taylor's (second order) theorem states that if function $f:U\rightarrow\mathbb{R}$ is $C^2$ and $U$ is an open subset of $\mathbb{R}^n$, then $\forall\mathbf{\overrightarrow{x}},\mathbf{\overrightarrow{a}}\in U:f(\mathbf{\overrightarrow{x}})=f(\mathbf{\overrightarrow{a}})+Df(\mathbf{\overrightarrow{a}})(\mathbf{\overrightarrow{x}}-\mathbf{\overrightarrow{a}})+\frac{1}{2}(\mathbf{\overrightarrow{x}}-\mathbf{\overrightarrow{a}})^TD^2f(\mathbf{\overrightarrow{a}})(\mathbf{\overrightarrow{x}}-\mathbf{\overrightarrow{a}})+R_2(\mathbf{\overrightarrow{x}},\mathbf{\overrightarrow{a}})$ where $\lim_{\mathbf{\overrightarrow{x}}\rightarrow\mathbf{\overrightarrow{a}}}\frac{R_2(\mathbf{\overrightarrow{x}},\mathbf{\overrightarrow{a}})}{||\mathbf{\overrightarrow{x}}-\mathbf{\overrightarrow{a}}||^2}\rightarrow0$.
	- Informally, any $f(\mathbf{\overrightarrow{x}})$ is well approximated by $f(\mathbf{\overrightarrow{a}})+Df(\mathbf{\overrightarrow{a}})(\mathbf{\overrightarrow{x}}-\mathbf{\overrightarrow{a}})+\frac{1}{2}(\mathbf{\overrightarrow{x}}-\mathbf{\overrightarrow{a}})^TD^2f(\mathbf{\overrightarrow{a}})(\mathbf{\overrightarrow{x}}-\mathbf{\overrightarrow{a}})$.

## Implicit Differentiation
- **The implicit function theorem states that if (1) $(\mathbf{\overrightarrow{x}}^*,y^*)$ solves $f(\mathbf{\overrightarrow{x}},y)=0$, (2) $f$ is $C^1$ in an open ball around $(\mathbf{\overrightarrow{x}}^*,y^*)$, and (3) $f_y(\mathbf{\overrightarrow{x}}^*,y^*)\neq0$, then there is a $C^1$ function $g(\mathbf{\overrightarrow{x}})$ defined on an open ball around $\mathbf{\overrightarrow{x}}^*$ such that $y^*=g(\mathbf{\overrightarrow{x}}^*)$, $f(\mathbf{\overrightarrow{x}},g(\mathbf{\overrightarrow{x}}))=0$, and $g_i(\mathbf{\overrightarrow{x}})=-\frac{f_i(\mathbf{\overrightarrow{x}},y)}{f_y(\mathbf{\overrightarrow{x}},y)}$. In other words, there is some function $g(\mathbf{\overrightarrow{x}})$ such that each $(\mathbf{\overrightarrow{x}},g(\mathbf{\overrightarrow{x}}))$ (in an open ball around $\mathbf{\overrightarrow{x}}^*$) solves $f(\mathbf{\overrightarrow{x}},y)=0$, and the change in $y$ from $y^*$ corresponding to a change in $\mathbf{\overrightarrow{x}}$ from $\mathbf{\overrightarrow{x}}^*$ can be found by implicit differentiation.**
	- **Note the negative sign on the left hand side of the equation for $g_i$.**
- The implicit function theorem generalises as follows. If (1) $(\mathbf{\overrightarrow{x^*}},\mathbf{\overrightarrow{y^*}})$ solves $f(\mathbf{\overrightarrow{x}},\mathbf{\overrightarrow{y}})=\mathbf{\overrightarrow{0}}$, (2) $f$ is $C^1$ in an open ball around $(\mathbf{\overrightarrow{x^*}},\mathbf{\overrightarrow{y^*}})$, and (3) $D_{\mathbf{\overrightarrow{y}}}f$ is invertible at $(\mathbf{\overrightarrow{x^*}},\mathbf{\overrightarrow{y^*}})$, then there is a $C^1$ function $g(\mathbf{\overrightarrow{x}})$ defined on an open ball around $\mathbf{\overrightarrow{x^*}}$ such that $\mathbf{\overrightarrow{y^*}}=g(\mathbf{\overrightarrow{x^*}})$, $f(\mathbf{\overrightarrow{x}},g(\mathbf{\overrightarrow{x}}))=0$, and $D_{\mathbf{\overrightarrow{x}}}g=-[D_{\mathbf{\overrightarrow{y}}}f(\mathbf{\overrightarrow{x}},g(\mathbf{\overrightarrow{x}}))]^{-1}D_{\mathbf{\overrightarrow{x}}}f(\mathbf{\overrightarrow{x}},g(\mathbf{\overrightarrow{x}}))$, i.e. $D_{\mathbf{\overrightarrow{x}}}g$ solves $AX=B$ where $A$ is the Jacobian (in $\mathbf{\overrightarrow{y}}$) of $f$ and $B$ is the Jacobian (in $\mathbf{\overrightarrow{x}}$) of $f$, each evaluated at $(\mathbf{\overrightarrow{x}},g(\mathbf{\overrightarrow{x}}))$.
	- **Note the negative sign on the left hand side of the equation for $D_{\mathbf{\overrightarrow{x}}}g$.**